{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "\n",
    "## Overview of our assignment\n",
    "* Given a face image dataset, namely CelebA, you need to perform classification with it, using an MLP and multiple CNN architectures, and then report the results.\n",
    "* We provide three models for classification, such as MLP, VGG, and ResNet-18, which can be used as the base architectures for your assignment.\n",
    "* You should use the following three regularization techniques: 1) Dropout, 2) L2 normalization, and 3) L1 normalization, based upon the provided three models, and then report the results with regularization techniques that you will implement. \n",
    "* Also, you should apply the following three optimization methods: 1) SGD with Momentum, 2) AdaGrad, and 3) Adam, and then report the obtained results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the CelebA dataset\n",
    "\n",
    "Note that the ~200,000 CelebA face image dataset is relatively large (~1.3 Gb). The download link provided below was provided by the author on the official CelebA website at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html. \n",
    "\n",
    "Download link: https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8\n",
    "\n",
    "1) Download and unzip the file `./Img/img_align_celeba.zip`, which contains the images in jpeg format.\n",
    "\n",
    "2) Download the `./Anno/list_attr_celeba.txt` file, which contains the class labels.\n",
    "\n",
    "3) Download the `./Eval/list_eval_partition.txt` file, which contains  training/validation/test partitioning info.\n",
    "\n",
    "Please make sure that, all your downloaded files (or unzipped folders) are located inside of the \"./data\" folder to run below codes.\n",
    "\n",
    "For example,\n",
    "* ./data/list_attr_celeba.txt\n",
    "* ./data/list_eval_partition.txt\n",
    "* ./data/img_align_celeba/{IMAGE_NAME}.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/list_attr_celeba.txt', sep=\"\\s+\", skiprows=1, usecols=['Male'])\n",
    "\n",
    "# Make 0 (female) & 1 (male) labels instead of -1 & 1\n",
    "df1.loc[df1['Male'] == -1, 'Male'] = 0\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./data/list_eval_partition.txt', sep=\"\\s+\", skiprows=0, header=None)\n",
    "df2.columns = ['Filename', 'Partition']\n",
    "df2 = df2.set_index('Filename')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.merge(df2, left_index=True, right_index=True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('./data/celeba-gender-partitions.csv')\n",
    "df4 = pd.read_csv('./data/celeba-gender-partitions.csv', index_col=0)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4['Partition'] == 0].to_csv('./data/celeba-gender-train.csv')\n",
    "df4.loc[df4['Partition'] == 1].to_csv('./data/celeba-gender-valid.csv')\n",
    "df4.loc[df4['Partition'] == 2].to_csv('./data/celeba-gender-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample image\n",
    "img = Image.open('./data/img_align_celeba/000001.jpg')\n",
    "print(np.asarray(img, dtype=np.uint8).shape)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the DataLoader for training, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=0)\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df.index.values\n",
    "        self.y = df['Male'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that transforms.ToTensor()\n",
    "# already divides pixels by 255. internally\n",
    "\n",
    "custom_transform = transforms.Compose([transforms.CenterCrop((178, 178)),\n",
    "                                       transforms.Resize((32, 32)),\n",
    "                                       #transforms.Grayscale(),                                       \n",
    "                                       #transforms.Lambda(lambda x: x/255.),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CelebaDataset(csv_path='./data/celeba-gender-train.csv',\n",
    "                              img_dir='./data/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "valid_dataset = CelebaDataset(csv_path='./data/celeba-gender-valid.csv',\n",
    "                              img_dir='./data/img_align_celeba/',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "test_dataset = CelebaDataset(csv_path='./data/celeba-gender-test.csv',\n",
    "                             img_dir='./data/img_align_celeba/',\n",
    "                             transform=custom_transform)\n",
    "\n",
    "BATCH_SIZE=256\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=6)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          num_workers=6)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing dataloader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "random_seed = 42\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "\n",
    "# Architecture\n",
    "num_features = 32*32\n",
    "num_classes = 2\n",
    "\n",
    "# Results\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import mlp\n",
    "from models import vgg\n",
    "from models import resnet\n",
    "#from models import dropout_mlp\n",
    "#from models import dropout_vgg\n",
    "#from models import dropout_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to specify the model that you want to use in here.\n",
    "\n",
    "#model = mlp.MLP()                 # For MLP\n",
    "model = vgg.VGG16()               # For VGG\n",
    "#model = resnet.ResNet18()         # For ResNet18\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to implement optimizers for this assignment,\n",
    "# Note that you can directly load the optimizer that you want to use in PyTorch library\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #SGD\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #ADAM\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.1) #ADAM w L2 norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "#        ### LOGGING\n",
    "#        if not batch_idx % 50:\n",
    "#            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "#                   %(epoch+1, num_epochs, batch_idx, \n",
    "#                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        train_acc = compute_accuracy(model, train_loader)\n",
    "        val_acc = compute_accuracy(model, valid_loader)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        costs.append(cost)\n",
    "        print('Epoch: %03d/%03d |Cost: %.3f | Train: %.3f%% | Valid: %.3f%%' % (\n",
    "              epoch+1, num_epochs, cost, train_acc, val_acc))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc = compute_accuracy(model, test_loader)\n",
    "    print('Test accuracy: %.2f%%' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "\n",
    "    features = features\n",
    "    targets = targets\n",
    "    break\n",
    "    \n",
    "plt.imshow(np.transpose(features[0], (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "logits, probas = model(features.to(device)[0, None])\n",
    "print('Probability Female %.2f%%' % (probas[0][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You need to draw training curves of your models, and then insert the curves as well as the final results of your model in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs_np = []\n",
    "\n",
    "for a in train_accs :\n",
    "    train_accs_np.append(a.cpu().numpy().item(0))\n",
    "    print(a.cpu().numpy().item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs_np = []\n",
    "\n",
    "for a in val_accs :\n",
    "    val_accs_np.append(a.cpu().numpy().item(0))\n",
    "    print(a.cpu().numpy().item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_np = []\n",
    "\n",
    "for a in costs :\n",
    "    costs_np.append(a.cpu().detach().numpy().item(0))\n",
    "    print(a.cpu().detach().numpy().item(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams['figure.figsize'] = [100, 80]\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "x = np.linspace(0, 200, 200)\n",
    "ax.plot(x, train_accs_np, label='train acc')\n",
    "ax.plot(x, val_accs_np, label='val acc')\n",
    "#ax.plot(x, test_accs_np, label='test acc')\n",
    "ax.set_xlabel('epochs')  \n",
    "ax.set_ylabel('accuracy') \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "x = np.linspace(0, 200, 200)\n",
    "ax.plot(x, costs_np, label='cost')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('cost') \n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "3f266c41d07c302057bd536a9dd31e4a772155858470f8dfb91085d93df1aae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
